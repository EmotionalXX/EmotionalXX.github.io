---
layout:     post
title:      [AI工程师]
subtitle:   人工智能
date:       2020-02-20
author:     Mr.Huang
header-img: img/2020.2.20.jpg
catalog: true
tags:
    - 人工智能
    - 机器学习
    - 深度学习
    
---
# 前言

感谢星环科技免费提供的第28期AI工程师认证培训课程，受益无穷！

# 学习内容

	 机器学习建模流程和思路、算法整理、数据预处理、分类回归、树类模型、聚类算法、推荐模型、机器学习模型评估与应用
		
## 机器学习建模流程和思路

机器学习建模流程可按照以下流程：
![3muTKI.jpg](https://s2.ax1x.com/2020/02/20/3muTKI.jpg)

**机器学习建模流程**：数据处理阶段、模型训练阶段和模型应用阶段

**数据处理阶段**：数据获取（理解业务和需求确认）、数据清洗（提升数据质量）、特征工程（提取特征，提升训练模型的效果）、数据整理（整合形成大宽表）

**数据训练阶段**：对之前加工的特征进行描述，评估其权重，必要的时候进行特征降维，之后结合具体的业务问题，选择一个或者几个机器学习算法，通过不断调整
参数反复训练模型，直到模型效果达到预期为止，然后要加载测试数据进行测试，防止发生过拟合问题而全然不知的情况发生。

**数据应用阶段**：在模型经过评估且效果能达到预期之后，我们需要将模型进行部署上线，并且对于线上的测试结果结合实际业务进行解释，看是否能达到业务部门
的目标，以及确认模型的迭代方式和迭代周期。

## 算法整理

**常见时间序列模型**：ARMA模型、ARIMA模型、RNN、LSTM、状态空间模型及Kalman滤波
优点：简单容易实现、快速展示分析成果。
缺点：模型只考虑了时间这一个特征，实际情况中可以能还跟其他特征有着密切的关系，比如是否是节假日，是否有重大活动以及天气情况等。

**常见回归算法**：lasso、ridge、randomforest、xgboost

**常见聚类算法**：Kmeans、混合高斯及层次聚类模型

**机器学习思路**：快速搭建机器学习框架，调优在大框架内逐步完成，先使用简单模型做出效果，然后逐步向业务贴近调整

## 数据预处理

**为什么要进行数据清洗？**
不同行业不同业务场景下产生的数据质量不一，存在编码问题、单位问题、格式问题、缺省值问题等，无法直接使用。
（大概）电力行业的数据质量最高，电商的数据质量最低

**数据清洗的时间**
一般发生在项目初期，同时也伴随着整个项目周期。具体来说，训练人工智能模型的数据源一般来自于数据仓库，而在构建数据仓库的
数据集成阶段会进行一次数据清洗工作，然后在数据建模项目开始的初期会进行数据清洗工作，在模型训练的任何阶段，
只要发现数据异常，都需要进行数据清洗工作。

**数据清洗的作法**
![3mlGJU.png](https://s2.ax1x.com/2020/02/20/3mlGJU.png)
去除或者修改格式和内容有错误的数据、去除或者修改逻辑有错误的数据、去除不需要的数据以及验证数据字段的关联性。

### 数据样本处理

数据样本处理可分为数据采样和样本切分和复制

数据采样，一般在以下两种情况进行，数据倾斜、数据量大（如男女比例为10:1，则需从男女生中合理采样，达到平衡）

数据切分和复制：原始数据80%作为训练集，20%作为测试集。

### 特征工程

数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。

属性是一个数据字段，表述数据对象的一个特征，用来描述一个给定对象的一组属性称为**特征向量**

**特征处理**

![3mY9sg.jpg](https://s2.ax1x.com/2020/02/20/3mY9sg.jpg)


### 衍生特征加工

机器学习建模的过程中最重要的就是特征工程，而特征工程中最重要的就是衍生特征的加工

**衍生特征**：称之为从原始数据中提出人们可识别的特征。
**静态特征**：代表用户不易发生改变的行为习惯
**动态特征**：代表用户最近的一些行为特征，也是模型训练中比较关系的部分。
**组合特征**：通过将部分特征进行组合处理或特征筛选，降低模型复杂度的同时保持模型效果。

### 基于权重的特征选取

利用评价函数选取前10%或前10个特征：

	Person相关系数
	Gini-index基尼系数
	IG信息增益、互信息
	Distance Metrics 距离度量

 PS：通过随机森林算法可得到输出参与模型训练的特征的权重

优点：计算时间上较为高效，对于过拟合问题具有高鲁棒性

缺点：倾向于选择冗余的特征；因为不考虑特征之间的相关性，导致某一个特征分类能力很差，但是和其他特征组合起来效果不错，被筛选掉。（多重共线性）

### 自动特征学习

在实际工程项目中，常常会遇到非结构化数据，通过人为经验很难得到有用的特征，需要借助于自动特征学习的方法

#### 自动特征学习-CNN

卷积神经网络CNN，它可以通过多个神经层，多次特征学习，来提取出一些通过人力无法提取的特征，从而提高基于这类数据源训练的模型的效果。


### 分类回归

### 线性回归

训练模型的过程中，就是用多组（x,y）的值来确定自变量x的参数和常数项的过程。

通过训练集训练得到学习算法，对于新的面积x，则代入算法可求解

**模型定义**

![3m0fgI.jpg](https://s2.ax1x.com/2020/02/20/3m0fgI.jpg)

通过代价函数求解模型权重和偏置量的两种方法：最小二乘法和梯度下降法

**最小二乘法**

![3mBYsP.jpg](https://s2.ax1x.com/2020/02/20/3mBYsP.jpg)

使用参数估计，对w和b求偏导数，即求S最小值。但是在数据量大时无能为力，本质上是无法将大批量数据加载到内存中进行
计算。

**梯度下降法**

梯度下降法可分批次，一轮一轮地计算求解。

梯度下降法的相当于我们下山的过程，每次我们要走一一步下山，寻找最低的地方，那么最可靠的方法便是环顾四周，寻找能--步到达的最低点，持续该过程，最后得到的便是最低点。对于函数而言，便是求得该函数对所有参数(变量)的偏导，每次更新这些参数，直到到达最低点为止，注意这些参数必须在每一轮一起更新， 而不是-一个一个更新。

#### 学习率

![3mDste.jpg](https://s2.ax1x.com/2020/02/20/3mDste.jpg)

#### 线性回归-数据属性转换

对于有序关系的离散属性，可将其通过索引化过程转化为可以比较大小的数字类型，从而参与线性回归模型训练；

对于不可比较大小的离散属性，可将其进行One-Hot编码将其转化为多元向量的形式参与模型训练。

常使用均方差MSE作为模型评估判断模型的效果，MSE越小，代表预测结果和真实结果的误差越小，模型效果越好

### 逻辑回归

逻辑回归是一种广义线性回归，实际上是一种分类器。逻辑回归=线性回归+sigmod函数


### 逻辑回归多分类

1.对于每个类别建立一个二分类器，有此类别的样本标记为1，其他类别样本标记为0
2.修改logistic回归的损失函数，softmax回归

### SVM

支持向量机是一种二分类模型，其基本思想是：对于给定的数据集D，在样本空间中找到一个划分超平面，从而将不同类别的样本分开。

SVM可使用核技巧有效地进行非线性分类，将输入隐式映射到高维特征空间中。

#### 超平面和支持向量

**线性可分**，其实就是用一条线或者一个面，将两类点进行区分，如果区分得开就是线性可分，否则就是线性不可分。
假如是线性可分的话，我们需要通过某种方式找到这样一条线或者一个面，我们称之为**超平面**。
辅助我们找到超平面的一些点或者向量，我们称之为**支持向量**。

#### 带核的SVM

**核函数**：作用就是将在低维空间线性不可分的数据特征映射到高维空间，使其变的线性可分。有可能在高维空间中找到一个特殊的形状，
将高维空间中的数据点进行分类，本质上解决了更复杂的数据分割问题。

#### SVM优缺点及适用场景

**SVM优点**：
1.SVM在解决小样本，韭线性以及高维特征中表现出许多特有的优势。
2.SVM基于有限的样本信息在模型的复杂度和模型准确性之间寻求最佳折中，以获得最好的预测效果。

**SVM缺点**：
1.在数据量大的情况下运算复杂度高，不适合处理过大的数据
2.模型稳定性低，输入的微小变化会使得模型难以收敛。
3.SVM仅直接适用于二分类任务。因此，必须应用将多类任务减少到几个二元问题的算法。

**适用场景**：
目前支持向量机主要应用在模式识别领域中的文本识别，中文分类，人脸识别等，同时也应用到信息过滤等方面。

### 朴素贝叶斯

简单的概率分类器，朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。

#### 贝叶斯公式

![3mcRdx.jpg](https://s2.ax1x.com/2020/02/20/3mcRdx.jpg)

#### 贝叶斯优缺点

![3mcLwt.jpg](https://s2.ax1x.com/2020/02/20/3mcLwt.jpg)

显著优点是算法原理简单，模型训练效率高，对缺失值不太敏感；缺点是数据特征相互独立的假设在实际应用中难成立。

## 树类模型

### 决策树

![3n3Wt0.jpg](https://s2.ax1x.com/2020/02/21/3n3Wt0.jpg)

决策树是有监督的机器学习算法，分为模型训练阶段和模型预测阶段

### 权重量化

#### 熵值法

第一种量化权重的指标是熵，熵是对信息量的量化，熵越大，不确定性越大，信息量越大、熵是对特征的离散程度
进行描述，权重的定量表示，特征离散程度越大，权重越大，对综合评价的影响越大

#### 信息增益(IG)

![3n3Wt0.jpg](https://s2.ax1x.com/2020/02/21/3n3Wt0.jpg)

特征选择常擦使用信息增益，如果IG大的话，对于分类很关键。

#### 基尼指数

Gini指数是二元属性划分，指数越大，不纯度越大，越不容易区分，越容易分错。

对于离散属性，选择该属性产生的最小基尼指数的数值作为分裂信息

### 决策树学习算法

#### 决策树学习算法ID3

ID3是普通的决策树算法，选择特征中是基于**信息增益**的。

![3nYegK.jpg](https://s2.ax1x.com/2020/02/21/3nYegK.jpg)

#### 决策树学习算法C4.5

C4.5的算法过程与ID3类似，只是在选择特征时使用**信息增益比**。同时具有如下特性：

	用信息增益比来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；
	在树构造过程中需要进行剪枝；
	能够自动完成对连续属性的离散化处理；

优点：产生的分类规则易于理解，准确率较高

缺点：在构造树的过程中，需要对数据集进行多次扫描和排序，导致算法的低效

**当属性取值很多时最好选择C4.5算法，ID3得出的效果会非常差，因为使用信息增益划分时它倾向于取值多的属性。**

PS：C5.0是C4.5的商业化版本，主要改进就是用了boosting，可以用更少的memory，更小的rule set，但是能获得更高的准确率。

#### 决策树学习算法CART

CART算法是使用 **Gini Index** 选择特征的，

与ID3和C4.5有以下异同点：

![3uVddU.jpg](https://s2.ax1x.com/2020/02/21/3uVddU.jpg)

#### 过拟合与剪枝

决策树会经常遇到过拟合问题，即训练的模型在训练集上表现良好，在测试集上表现很差。通常的解决办法是剪枝，分为预剪枝和后剪枝。

具体原理如下：

![3uZdXt.jpg](https://s2.ax1x.com/2020/02/21/3uZdXt.jpg)

### Bagging和随机森林

#### 集成学习思想

通过将弱学习器经过某些特定的方式组合成为强学习器，常见学习方法有集成学习、强化学习和深度学习。

在机器学习建模过程中，利用集成学习思想，我们会训练多个独立的模型，共同来完成真理的学习，每个模型只会学习真理的一部分，
最终对多个模型进行综合评估来提升模型效果。

装袋法Bagging和Boosting是集成学习思想的典型体现。Bagging的代表算法是随机森林，而Boosting的代表算法有Adaboost、GBDT、XGBoost。

#### 集成学习方法-Bagging

Bagging（装袋法）是一个统计重采样的技术，基础是Bootstrap。

**基本思想:** 利用Bootstrap方法重采样来生成多个版本的预测分类，然后把这些分类器进行组合。
可重复的随机采用技术Bootstrap采用重复随机采样获得的样本得到没有或较少的噪声数据。

Booststrap采样方式表明，会有三分之一的数据不会被采样为训练数据，而可以作为测试数据对模型效果进行验证和评估，能有效的避免模型训练过程中的过拟合问题。

#### Bagging算法流程

![3uM4BR.jpg](https://s2.ax1x.com/2020/02/21/3uM4BR.jpg)

#### Bagging-随机森林

**核心思想**：训练多棵互相不关联的决策树，来共同决策。

**具体做法**：

![3ulxtP.jpg](https://s2.ax1x.com/2020/02/21/3ulxtP.jpg)

随机森林可做分类和回归，对于分类问题，常常采用投票的方式决定最终判定的类别，少数服从多数。

![3u3U5q.jpg](https://s2.ax1x.com/2020/02/21/3u3U5q.jpg)

#### 随机森林-优缺点

**优点**：随机森林算法，因为会对样本和特征进行采样来构建多棵树，因此可以应用大规模样本集和高维特征的情况，也不容易陷入过拟合，也不用做特征筛选。
在随机森林中，训练多棵树的过程可以并行化，因此，随机森林算法的性能可以得到保证，但是一次需要训练多棵树，相对于决策树，需要更多的计算资源。
因为随机森林中每棵树的构建过程中都是基于某种特征的策略选择特征的，因此模型可以将特征的权重进行输出。同时也能检测到特征之间的多重贡献性
，以及应对正负样本不平衡的问题。

**缺点**：对于样本规模小，且低维的训练数据集，就很难发挥随机森林的优势，因此就不一定会得到比较好的模型效果了

### Adaboost与GDBT（梯度提升树）

#### 集成学习方法-Boosting

Boosting是一种通过组合弱学习器来产生强学习器的通用且有效的方法，提高弱分类器准确度。与Bagging相比，Bagging是让多个
弱学习器同时学习真理，每个只学习真理的一部分，而Adaboost是让每一个弱学习器充分学习真理，只不过后一个在前一个的基础上学习。

#### Adaboost

Adaboost是Adaptive Boosting，自适应增强

**自适应**在于：前一个基本分类器分错的样本得到加强，加权后的全体样本再次被用来训练下一个基本分类器，同时，在每一轮中加入
一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。

**Adaboost迭代算法**

![3udxSA.jpg](https://s2.ax1x.com/2020/02/21/3udxSA.jpg)

#### 各种树模型的优缺点

![3uBMiF.jpg](https://s2.ax1x.com/2020/02/21/3uBMiF.jpg)

# 结语

关于**星环科技-AI工程师培训认证**的学习差不多在此结尾，后续有补充，将会继续更新。

 
 > 本文首次发布于 [Mr.Huang Blog](http://www.huangsz.xyz), 作者 [@(Mr.Huang)](http://github.com/EmotionalXX) ,转载请保留原文链接.







