---
layout:     post
title:      [AI工程师]
subtitle:   人工智能
date:       2020-02-20
author:     Mr.Huang
header-img: img/2020.2.20.jpg
catalog: true
tags:
    - 人工智能
    - 机器学习
    - 深度学习
    
---
# 前言

感谢星环科技免费提供的第28期AI工程师认证培训课程，受益无穷！

# 学习内容

	 机器学习建模流程和思路、算法整理、数据预处理、分类回归、树类模型、聚类算法、推荐模型、机器学习模型评估与应用
		
## 机器学习建模流程和思路

机器学习建模流程可按照以下流程：
![3muTKI.jpg](https://s2.ax1x.com/2020/02/20/3muTKI.jpg)

**机器学习建模流程**：数据处理阶段、模型训练阶段和模型应用阶段

**数据处理阶段**：数据获取（理解业务和需求确认）、数据清洗（提升数据质量）、特征工程（提取特征，提升训练模型的效果）、数据整理（整合形成大宽表）

**数据训练阶段**：对之前加工的特征进行描述，评估其权重，必要的时候进行特征降维，之后结合具体的业务问题，选择一个或者几个机器学习算法，通过不断调整
参数反复训练模型，直到模型效果达到预期为止，然后要加载测试数据进行测试，防止发生过拟合问题而全然不知的情况发生。

**数据应用阶段**：在模型经过评估且效果能达到预期之后，我们需要将模型进行部署上线，并且对于线上的测试结果结合实际业务进行解释，看是否能达到业务部门
的目标，以及确认模型的迭代方式和迭代周期。

## 算法整理

**常见时间序列模型**：ARMA模型、ARIMA模型、RNN、LSTM、状态空间模型及Kalman滤波
优点：简单容易实现、快速展示分析成果。
缺点：模型只考虑了时间这一个特征，实际情况中可以能还跟其他特征有着密切的关系，比如是否是节假日，是否有重大活动以及天气情况等。

**常见回归算法**：lasso、ridge、randomforest、xgboost

**常见聚类算法**：Kmeans、混合高斯及层次聚类模型

**机器学习思路**：快速搭建机器学习框架，调优在大框架内逐步完成，先使用简单模型做出效果，然后逐步向业务贴近调整

## 数据预处理

**为什么要进行数据清洗？**
不同行业不同业务场景下产生的数据质量不一，存在编码问题、单位问题、格式问题、缺省值问题等，无法直接使用。
（大概）电力行业的数据质量最高，电商的数据质量最低

**数据清洗的时间**
一般发生在项目初期，同时也伴随着整个项目周期。具体来说，训练人工智能模型的数据源一般来自于数据仓库，而在构建数据仓库的
数据集成阶段会进行一次数据清洗工作，然后在数据建模项目开始的初期会进行数据清洗工作，在模型训练的任何阶段，
只要发现数据异常，都需要进行数据清洗工作。

**数据清洗的作法**
![3mlGJU.png](https://s2.ax1x.com/2020/02/20/3mlGJU.png)
去除或者修改格式和内容有错误的数据、去除或者修改逻辑有错误的数据、去除不需要的数据以及验证数据字段的关联性。

#### 缺失值清洗

![3KnLQJ.jpg](https://s2.ax1x.com/2020/02/21/3KnLQJ.jpg)


### 数据样本处理

数据样本处理可分为数据采样和样本切分和复制

#### 数据采样

数据采样，一般在以下两种情况进行，数据倾斜（样本权重不一样）、数据量大

1、欠采样，将比例较多的样本数量降下来，保持平衡。

导致整体数据量下降，可通过集成思想boosting人为扩充样本

2、过采样，将比例较少的样本数量升上去，保持平衡

3、去除标签，作无监督学习

#### 数据切分

数据切分：原始数据80%作为训练集，20%作为测试集。

数据复制：容易出现过拟合情况

### 特征工程

#### 机器学习与特征工程的关联

数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已

#### 数据的基本属性

属性是一个数据字段，表述数据对象的一个特征

**特征向量**：用来描述一个给定对象的一组属性

标称属性：标称属性的值是一些符号或事物的名称，每种值代表某种类别、编码或状态

序数属性：可能的值直接具有意义的顺序或排序等级，但是相继值之间的差是未知的

离散属性：具有有限或无线可能个值

连续属性：非离散属性，在实际应用中，常常用float变量表示

#### 特征处理

![3mY9sg.jpg](https://s2.ax1x.com/2020/02/20/3mY9sg.jpg)

#### 类别特征处理

向量有两种表示方式：密集向量和稀疏向量

color取值,密集向量表示，稀疏向量表示

red,(1,0,0),(3,[0],[1])

green,(0,1,0),(3,[1],[1])

#### 日期型特征处理

比如将日期2014-09-20 20:45:40转化为有用的特征。

建立回归模型或序数特征。研究周几对交通的影响，可采用one-hot编码

#### 文本型特征处理

采用Word2vector方法或TF-IDF方法

#### 组合特征分析

通过不同特征进行组合产生新的特征

#### 衍生特征加工

机器学习建模的过程中最重要的就是特征工程，而特征工程中最重要的就是衍生特征的加工

**衍生特征**：称之为从原始数据中提出人们可识别的特征。

**静态特征**：代表用户不易发生改变的行为习惯

**动态特征**：代表用户最近的一些行为特征，也是模型训练中比较关系的部分。

**组合特征**：通过将部分特征进行组合处理或特征筛选，降低模型复杂度的同时保持模型效果。

#### 基于权重的特征选取

利用评价函数选取前10%或前10个特征：

	Person相关系数
	Gini-index基尼系数
	IG信息增益、互信息
	Distance Metrics 距离度量

 PS：通过随机森林算法可得到输出参与模型训练的特征的权重

优点：计算时间上较为高效，对于过拟合问题具有高鲁棒性

缺点：倾向于选择冗余的特征；因为不考虑特征之间的相关性，导致某一个特征分类能力很差，但是和其他特征组合起来效果不错，被筛选掉。（多重共线性）

#### 自动特征学习

在实际工程项目中，常常会遇到非结构化数据，通过<u>人为经验</U>很难得到有用的特征，需要借助于自动特征学习的方法

#### 自动特征学习-CNN

卷积神经网络CNN，它可以通过多个神经层，多次特征学习，来提取出一些通过人力无法提取的特征，从而提高基于这类数据源训练的模型的效果。

#### 算法选择导图

![3Kt1eS.jpg](https://s2.ax1x.com/2020/02/21/3Kt1eS.jpg)


### 线性回归

训练模型的过程中，就是用多组（x,y）的值来确定自变量x的参数和常数项的过程。

通过训练集训练得到学习算法，对于新的面积x，则代入模型可求解

目标是求得适当的w和b，使得函数S最小

**模型定义**

通过代价函数求解模型权重和偏置量的两种方法：最小二乘法和梯度下降法

#### 最小二乘法

![3mBYsP.jpg](https://s2.ax1x.com/2020/02/20/3mBYsP.jpg)

使用参数估计，对w和b求偏导数，即求S最小值。

**缺陷：**但是在数据量大时无能为力，本质上是无法将大批量数据加载到内存中进行计算，内存溢出。

#### 梯度下降法

梯度下降法可分批次，一轮一轮地计算求解。

梯度下降法的相当于我们下山的过程，每次我们要走一一步下山，寻找最低的地方，那么最可靠的方法便是环顾四周，寻找能--步到达的最低点，持续该过程，最后得到的便是最低点。对于函数而言，便是求得该函数对所有参数(变量)的偏导，每次更新这些参数，直到到达最低点为止，注意这些参数必须在每一轮一起更新， 而不是-一个一个更新。

#### 学习率

![3mDste.jpg](https://s2.ax1x.com/2020/02/20/3mDste.jpg)

#### 线性回归-数据属性转换

对于有序关系的离散属性，可将其通过索引化过程转化为可以比较大小的数字类型，从而参与线性回归模型训练；

对于不可比较大小的离散属性，可将其进行One-Hot编码将其转化为多元向量的形式参与模型训练。

常使用均方差MSE作为模型评估判断模型的效果，MSE越小，代表预测结果和真实结果的误差越小，模型效果越好

**注意：**使用MSE评估模型效果时，需要feature scaling(特征缩放)，如min-max归一化

### 逻辑回归

逻辑回归是一种广义线性回归，实际上是一种**分类器**。逻辑回归=线性回归+sigmod函数

逻辑回归支持二分类和多分类问题，支持连续和类别特征，但类别特征在字符串索引后需要进行**one-hot**处理

### 逻辑回归多分类

1.对于每个类别建立一个二分类器，有此类别的样本标记为1，其他类别样本标记为0

2.修改logistic回归的损失函数，softmax回归,多次判别进行分类

### SVM

支持向量机是一种二分类模型，其基本思想是：对于给定的数据集D，在样本空间中找到一个划分超平面，从而将不同类别的样本分开。

SVM可使用核技巧有效地进行非线性分类，将输入隐式映射到高维特征空间中。

#### 超平面和支持向量

**线性可分**，其实就是低维空间中用一条线或一个面，将两类点进行区分，如果区分得开就是线性可分，否则就是线性不可分。

![3KTwXq.jpg](https://s2.ax1x.com/2020/02/22/3KTwXq.jpg)

假如是线性可分的话，我们需要通过某种方式找到这样一条线或者一个面，我们称之为**超平面**。

超平面在一维空间中是点，二维空间中是线，在3维以上是面。

辅助我们找到超平面的一些点或者向量，我们称之为**支持向量**。

#### 带核的SVM

**核函数**：作用就是将在低维空间线性不可分的数据特征映射到高维空间，使其变的线性可分。有可能在高维空间中找到一个特殊的形状，
将高维空间中的数据点进行分类，本质上解决了更复杂的数据分割问题。

#### SVM优缺点及适用场景

**SVM优点**：
1.SVM在解决**小样本，非线性以及高维特征**中表现出许多特有的优势。
2.SVM基于有限的样本信息在模型的复杂度和模型准确性之间寻求最佳折中，以获得最好的预测效果。

**SVM缺点**：
1.在数据量大的情况下运算复杂度高，不适合处理过大的数据
2.模型稳定性低，输入的微小变化会使得模型难以收敛。
3.SVM仅直接适用于二分类任务。因此，必须应用将多类任务减少到几个二元问题的算法。

**适用场景**：
目前支持向量机主要应用在模式识别领域中的文本识别，中文分类，人脸识别等，同时也应用到信息过滤等方面。

### 朴素贝叶斯

简单的概率分类器，朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。

#### 贝叶斯公式

![3mcRdx.jpg](https://s2.ax1x.com/2020/02/20/3mcRdx.jpg)

#### 贝叶斯优缺点

![3mcLwt.jpg](https://s2.ax1x.com/2020/02/20/3mcLwt.jpg)

显著优点是算法原理简单，模型训练效率高，对缺失值不太敏感；缺点是数据特征相互独立的假设在实际应用中难成立。

## 树类模型

### 决策树

![3n3Wt0.jpg](https://s2.ax1x.com/2020/02/21/3n3Wt0.jpg)

决策树是有监督的机器学习算法，分为模型训练阶段和模型预测阶段

### 权重量化

评价特征重要性的指标：熵、信息增益、基尼指数

#### 熵值法

第一种量化权重的指标是熵，熵是对信息量的量化，熵越大，不确定性越大，信息量越大。

熵是对特征的离散程度进行描述，权重的定量表示，特征离散程度越大，权重越大，对综合评价的影响越大。

熵权法是一种客观赋权法，依赖于数据本身的离散性

**条件熵** 在特定变量发生的条件下指定变量发生与否的熵

#### 信息增益(IG)

![3n3Wt0.jpg](https://s2.ax1x.com/2020/02/21/3n3Wt0.jpg)

特征选择常擦使用信息增益，如果IG大的话，对于分类很关键。

信息增益指的是不确定性的减少程度，信息增益=信息熵-条件熵

数值越大，表示某个条件熵对信息熵减少程度越大，即对信息的判断起到的作用越大

#### 基尼指数

Gini指数是二元属性划分，指数越大，不纯度越大，越不容易区分，越容易分错。

对于离散属性，选择该属性产生的最小基尼指数的数值作为分裂信息

### 决策树学习算法

#### 决策树学习算法ID3

ID3是普通的决策树算法，选择特征中是基于**信息增益**的。

![3nYegK.jpg](https://s2.ax1x.com/2020/02/21/3nYegK.jpg)

#### 决策树学习算法C4.5

C4.5的算法过程与ID3类似，只是在选择特征时使用**信息增益比**。同时具有如下特性：

	用信息增益比来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；
	在树构造过程中需要进行剪枝；
	能够自动完成对连续属性的离散化处理；

优点：产生的分类规则易于理解，准确率较高

缺点：在构造树的过程中，需要对数据集进行多次扫描和排序，导致算法的低效

**当属性取值很多时最好选择C4.5算法，ID3得出的效果会非常差，因为使用信息增益划分时它倾向于取值多的属性。**

PS：C5.0是C4.5的商业化版本，主要改进就是用了boosting，可以用更少的memory，更小的rule set，但是能获得更高的准确率。

#### 决策树学习算法CART

CART算法是使用 **Gini Index** 选择特征的，

与ID3和C4.5有以下异同点：

![3uVddU.jpg](https://s2.ax1x.com/2020/02/21/3uVddU.jpg)

#### 过拟合与剪枝

决策树会经常遇到过拟合问题，即训练的模型在训练集上表现良好，在测试集上表现很差。通常的解决办法是剪枝，分为预剪枝和后剪枝。

具体原理如下：

![3uZdXt.jpg](https://s2.ax1x.com/2020/02/21/3uZdXt.jpg)

### Bagging和随机森林

#### 集成学习思想

通过将弱学习器经过某些特定的方式组合成为强学习器，常见学习方法有集成学习、强化学习和深度学习。

在机器学习建模过程中，利用集成学习思想，我们会训练多个独立的模型，共同来完成真理的学习，每个模型只会学习真理的一部分，
最终对多个模型进行综合评估来提升模型效果。

装袋法Bagging和Boosting是集成学习思想的典型体现。Bagging的代表算法是随机森林，而Boosting的代表算法有Adaboost、GBDT、XGBoost。

#### 集成学习方法-Bagging

Bagging（装袋法）是一个统计重采样的技术，基础是Bootstrap。

**基本思想:** 利用Bootstrap方法重采样来生成多个版本的预测分类，然后把这些分类器进行组合。

可重复的随机采用技术Bootstrap采用重复随机采样获得的样本得到没有或较少的噪声数据。

Booststrap采样方式表明，会有三分之一的数据不会被采样为训练数据，而可以作为测试数据对模型效果进行验证和评估，能有效的避免模型训练过程中的过拟合问题。

#### Bagging算法流程

![3uM4BR.jpg](https://s2.ax1x.com/2020/02/21/3uM4BR.jpg)

#### Bagging-随机森林

**核心思想**：训练多棵互相不关联的决策树，来共同决策。

**具体做法**：

![3ulxtP.jpg](https://s2.ax1x.com/2020/02/21/3ulxtP.jpg)

**特点**

随机森林的每棵树的训练样本是随机的，树中每个节点是随机的，树中每个节点的分裂属性也是随机选择的。即使每棵决策树没有剪枝，
随机森林也不会产生过拟合现象。

**注意**：需要认为给定两个超参数，森林中树的数量（一般取值较大）和m值大小（分类为根号下M，回归为M/3)

随机森林可做分类和回归，对于分类问题，常常采用投票的方式决定最终判定的类别，少数服从多数。

![3u3U5q.jpg](https://s2.ax1x.com/2020/02/21/3u3U5q.jpg)

#### 随机森林-优缺点

**优点**：随机森林算法，因为会对样本和特征进行采样来构建多棵树，因此可以应用大规模样本集和高维特征的情况，也不容易陷入过拟合，也不用做特征筛选。
在随机森林中，训练多棵树的过程可以并行化，因此，随机森林算法的性能可以得到保证，但是一次需要训练多棵树，相对于决策树，需要更多的计算资源。
因为随机森林中每棵树的构建过程中都是基于某种特征的策略选择特征的，因此模型可以将特征的权重进行输出。同时也能检测到特征之间的多重贡献性
，以及应对正负样本不平衡的问题。

**缺点**：对于样本规模小，且低维的训练数据集，就很难发挥随机森林的优势，因此就不一定会得到比较好的模型效果了

### Adaboost与GDBT（梯度提升树）

#### 集成学习方法-Boosting

Boosting是一种通过组合弱学习器来产生强学习器的通用且有效的方法，提高弱分类器准确度。与Bagging相比，Bagging是让多个
弱学习器同时学习真理，每个只学习真理的一部分，而Adaboost是让每一个弱学习器充分学习真理，只不过后一个在前一个的基础上学习。

#### Adaboost

Adaboost是Adaptive Boosting，自适应增强

**自适应**：前一个基本分类器分错的样本得到加强，加权后的全体样本再次被用来训练下一个基本分类器，同时，在每一轮中加入
一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。

**Adaboost迭代算法**

![3udxSA.jpg](https://s2.ax1x.com/2020/02/21/3udxSA.jpg)

**Adaboost优缺点**

![3Mjnu4.jpg](https://s2.ax1x.com/2020/02/22/3Mjnu4.jpg)


#### GBDT

全称是Gradient Boosting Decision Tree

GBDT特点：

1.本质上回归决策树，更适合做回归，也可以做二分类

2.GBDT是通过树的累加，核心思想是每个决策树只学习真理的一部分

3.Gradient Boot与传统Boost区别是，每一次的计算是为了减少上一次的残差。

**核心**：GBDT不仅仅是简单地运用集成思想，且它是基于之前所有树结论和的残差

#### XGBoost

XGBOOST是大规模并行boosted tree的工具，比常见的工具包快10倍以上。XGBoost里面的基学习器除了用tree(gbtree),
也可用线性分类器(gblinear)，GBDT特指梯度提升决策树算法


**XGBoost特点**

![3MjIP0.jpg](https://s2.ax1x.com/2020/02/22/3MjIP0.jpg)


#### 各种树模型的优缺点

![3uBMiF.jpg](https://s2.ax1x.com/2020/02/21/3uBMiF.jpg)

### 机器学习模型评估与优化

#### 偏差方差权衡

**偏差**代表了经过模型预测的结果与真实情况的接近程度，偏差较大，代表训练的模型效果不好

**方差**代表了模型复杂度，方差越大，模型越复杂，对问题本质的刻画越准确

#### 方差和偏差的三种情况

![3u4i7Q.jpg](https://s2.ax1x.com/2020/02/21/3u4i7Q.jpg)

**第一种情况**：模型方差太小，导致偏差太大，最终模型无法刻画问题本质，会出现<u>欠拟合</u>问题，
也就是说，模型在训练集上表现不好，在测试集上同样表现不好。

**第二种情况**：模型方差和偏差适中，能基本上刻画问题的本质，在训练集和测试集上表都能够接受，
这种正常在生产中是比较常见，也是比较合理的。

**第三种情况**：模型方差过大，偏差过小，能很好的客户训练数据集的本质，但是对测试集表现不好，
也是说，模型的泛化能力很差，这样的模型，我们称之为<u>过拟合</u>

#### 偏差方差权衡-总结

**过拟合**：模型在训练集上表现好（损失函数小），在测试集上表现差（损失函数大）。

**过拟合解决办法**：减小模型的复杂度，或增大正则因子lambda

**欠拟合**：模型在训练集上表现差，在测试集上表现差

**欠拟合解决办法**：增大模型的复杂度，或减小正则因子lambda

#### 交叉验证K-Fold

**交叉验证**就是将一部分训练数据用来测试模型效果，用于无测试集和模型参数不确定的情况。
也被称为K-fold Cross-Validation

**计算流程**
![3uTRRs.jpg](https://s2.ax1x.com/2020/02/21/3uTRRs.jpg)

#### 交叉验证LOO-CV

**LOO-CV**：如果设原始数据有N个样本，那么LOO-CV就是N-CV，即每个样本单独作为验证集，其余的N-1个样本作为训练集，
所以LOO-CV会得到N个模型，用这N个模型最终的验证集的分类准确率的平均数作为此下LOO-CV分类器的性能指标。

**LOO-CV优缺点**

![3uHEh4.jpg](https://s2.ax1x.com/2020/02/21/3uHEh4.jpg)

#### OOB验证

![3uHNjI.jpg](https://s2.ax1x.com/2020/02/21/3uHNjI.jpg)

经过OOB验证，k折交叉验证之后，会大概有36.8%的数据不会被采样为训练数据，而部分数据用来做测试数据验证模型，能有效的避免模型出现过拟合问题，
是训练出来的模型具有较好的泛化能力。

### 模型评价

#### 分类模型评价-精准率和召回率

![3ubPxA.jpg](https://s2.ax1x.com/2020/02/21/3ubPxA.jpg)

精确度，指的是在所有识别成正样本中有多少是真正的正样本。

召回率，指的是正样本中被识别出来的样本，分为正样本召回率和负样本召回率

#### 分类模型评价-召回率和精准率权衡

癌症检查中，希望召回率越高越好；浏览器搜索或垃圾邮件检测中，希望精确率越高越好。

#### 分类模型评价F-Score

在不知道使用召回率或精准度的时候，可以考虑F-score。综合精准度（Precision）和召回率（Recall）考虑

![3uxFfK.jpg](https://s2.ax1x.com/2020/02/21/3uxFfK.jpg)

#### 分类模型评价AUC/ROC

AUC是值，ROC是曲线，是二分类器的指标。

ROC是受试者工作特征曲线（receiver operating characteristic curve）的简写，又称为感受性曲线（sensitivity curve）；

AUC是ROC曲线下面积（Area Under roc Curve）简称，AUC的值是处于ROC curve下方的那部分面积的大小

**通常，AUC的值介于0.5-1之间，AUC越大，分类准确度越高。在ROC曲线上，最靠近坐标图左上方的点为敏感性和特异性较高的临界值**

![3KS9G6.jpg](https://s2.ax1x.com/2020/02/21/3KS9G6.jpg)

![3KSNiq.jpg](https://s2.ax1x.com/2020/02/21/3KSNiq.jpg)

#### 回归模型评价基尼系数、K-S曲线和KS值

![3Kpcng.jpg](https://s2.ax1x.com/2020/02/21/3Kpcng.jpg)

#### 回归模型评价

评价模型主要是误差计算，主要有MAE、MSE、RMSE和R方。

**区别**：除了R方能自动做归一化操作，其余需先进行归一化，再计算误差。

![3K9SgK.jpg](https://s2.ax1x.com/2020/02/21/3K9SgK.jpg)

#### 聚类模型评价

**聚类模型**属于非监督学习算法，聚类之后产生不同的簇，同一个簇内相异度低，相识度高，而不同簇之间相反。
评价聚类模型的聚类效果转变成计算簇内相异度的问题。

常见评价函数

![3uLcgH.jpg](https://s2.ax1x.com/2020/02/21/3uLcgH.jpg)

### 人工智能模型优化思路

#### 样本不均衡问题模型优化

![3K9RKO.jpg](https://s2.ax1x.com/2020/02/21/3K9RKO.jpg)

#### 交叉验证K-Fold

**交叉验证**就是将一部分训练数据用来测试模型效果，也被称为K-fold Cross-Validation。

通常是训练数据规模过小（通常小于1万条）或无测试集的情况下

#### 通过网格搜索进行参数调优

Grid Search：是一种调优手段，穷举搜索。在所有候选的参数选择中，通过循环遍历，尝试每一种可能性，
表现最好的参数是最终的结果，原理如同在数组里找最大值。


# 结语

关于**星环科技-AI工程师培训认证**的学习差不多在此结尾，后续有补充，将会继续更新。

 
 > 本文首次发布于 [Mr.Huang Blog](http://www.huangsz.xyz), 作者 [@(Mr.Huang)](http://github.com/EmotionalXX) ,转载请保留原文链接.







