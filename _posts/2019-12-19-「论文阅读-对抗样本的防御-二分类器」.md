---
layout:     post
title:      [对抗样本的防御——对抗检测——二分类器]
subtitle:   论文阅读
date:       2019-12-19
author:     Mr.Huang
header-img: img/2019.12.19.jpg
catalog: true
tags:
    - 论文阅读
    - 对抗样本
    - 对抗检测
    - 二分类器
    
---

# 前言

阅读论文《Adversarial and Clean Data Are Not Twins》

## 研究意义


	建立二分类器，作为预处理步骤，可直接部署在智能系统上，用于识别对抗样本
	
	证明该分类器在第二轮对抗攻击中是具有鲁棒性的
	
	证明所有防御方法有存在泛化限制，这是一个本质属性

## 介绍

**对抗样本**:通过在干净样本添加人为制造的噪音使其在视觉上有细微差别，但是在卷积神经网络中能以高置信度判别为另一类别。

以FGSM(Fast Gradient Sign Method)攻击的Mnist数据集为例：

[![QLEhRA.md.jpg](https://s2.ax1x.com/2019/12/19/QLEhRA.md.jpg)](https://imgchr.com/i/QLEhRA)

  + 第一行展示为干净样本的正确输出结果
  + 第二行展示为对抗样本输出结果


### 对抗样本的生成

+ 2013年Szegedy提出假设：对抗样本的存在是因为深度神经网络模型高度非线性的结果
+ 2014年Goodfellow推翻了Szegedy的假设，认为原因应该是过于线性。随后提出对抗样本的生成方法FGSM和LLCM(后来演变为TGSM,作者Kurakin）
+ 2015年Papernot提出JSMA（计算每个像素的显著性得分，选择得分最高的像素进行扰动），可将对抗样本的标签进行指定 
+ 2016年Szegedy指出对抗样本在不同神经网络模型之间具有迁移性
+ 2016年Papernot指出更糟糕的是对抗样本在不同机器学习技术上是可迁移的，如CNN、SVM和逻辑回归

### 对抗样本的防御

+ 2016年Kurakin指出一些图像变换，如高斯噪声、高斯滤波器和JPEG压缩可以有效恢复80%以上的对抗样本
+ Papernot提出防御性网络蒸馏
+ Kurakin提出对抗训练

---

## 实验操作——二分器

1.利用模型f1训练干净样本，然后通过攻击算法制作对抗样本

2.建立模型f2进行有监督训练，干净样本标记为0，对抗样本标记为1

3.测试干净样本和对抗样本

4.利用模型f2构建第二轮对抗测试数据

本论文中年的[实验代码](https://github.com/gongzhitaao/adversarial-classifier)

**实验结果**

[![QLae4H.md.jpg](https://s2.ax1x.com/2019/12/19/QLae4H.md.jpg)](https://imgchr.com/i/QLae4H)

实验表明：

+ 对于测试的二分类方法，达到超过99%的准确率，同时对第二轮对抗攻击是具有鲁棒性的。

+ FGSM与JSMA生成的对抗样本是不兼容的，而FGSM与TGSM生成的对抗样本是兼容的，即分类器训练FGSM对抗样本集，也能在TGSM对抗样本的分类上表现不错。

**泛化限制**
>利用FGSM/TGSM训练的分类器对超参数敏感
>对生成对抗样本的算法敏感

# 结语

 > 本文首次发布于 [Mr.Huang Blog](http://www.huangsz.xyz), 作者 [@(Mr.Huang)](http://github.com/EmotionalXX) ,转载请保留原文链接.
